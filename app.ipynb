{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
      "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
      "7  50e14c0bb8                                         Soooo high   \n",
      "8  e050245fbd                                        Both of you   \n",
      "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
      "\n",
      "                                       selected_text sentiment  \n",
      "0                I`d have responded, if I were going   neutral  \n",
      "1                                           Sooo SAD  negative  \n",
      "2                                        bullying me  negative  \n",
      "3                                     leave me alone  negative  \n",
      "4                                      Sons of ****,  negative  \n",
      "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
      "6                                                fun  positive  \n",
      "7                                         Soooo high   neutral  \n",
      "8                                        Both of you   neutral  \n",
      "9                       Wow... u just became cooler.  positive  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# df = df.drop([\"selected_text\",\"textID\"],axis=1)\n",
    "df['text'] = df['text'].replace(r'http\\S+', '', regex=True)\n",
    "df['text'] = df[\"text\"].replace(np.nan,\"aaaa\")\n",
    "print((df[\"text\"] == \"nan\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_token = [token for token in doc if token not in stop_words]\n",
    "    \n",
    "    return \" \".join([token.text for token in filtered_token])\n",
    "\n",
    "def get_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    filtred_alpha = [lemma.lower() for lemma in lemmas if lemma.isalpha()]\n",
    "    \n",
    "    return \" \".join(filtred_alpha)\n",
    "\n",
    "# df[\"tweet\"] = df[\"text\"].apply(get_lemma)\n",
    "\n",
    "\n",
    "def remove_noise(text):\n",
    "    text = remove_stop_words(text)\n",
    "    text = get_lemma(text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "df[\"tweet\"] = df[\"text\"].apply(remove_noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
